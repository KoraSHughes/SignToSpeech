# SignToSpeech
Personal Project Using CS-UY 4563 Machine Learning
By Kora S. Hughes

Paper: https://drive.google.com/file/d/1nJ3Z_PBy9S_16MsACai9DAsWS7SY6e9d/view?usp=sharing <br>
More Info: https://www.linkedin.com/posts/korashughes_algorithmic-classification-models-for-sign-activity-7007154391946305536-T1fT?utm_source=share&utm_medium=member_desktop <br>

## Abstract:
SignToSpeech builds on Google's MediaPipe hand tracking software and delivers a machine learning based translation software for American Sign Language (ASL) users.
The purpose is to reduce the need for human sign-langauge interpretors and overall increase the accessibility of communication for the deaf/hard-of-hearing community: a google-translate for sign language.<br>
The purpose of this algorithm is to build on existing sign language processing (SLP) techniques by testing the feasibility of single-hand-centered classification and evaulating which algorithms work the best.

## Data Usage:
- Homebrew Data: https://drive.google.com/drive/u/3/folders/10QZHo7gI2Y2C0V5OqWY9i8yvhxj5gkwz
- Kaggle Training Set: https://www.kaggle.com/datasets/ardamavi/27-class-sign-language-dataset

## Results Overview:
- Logistic Regresion:
- Support Vector Machines:
- Neural Networks

### SLAP (Sign Language Abstraction Picture) Visualizations:
https://observablehq.com/d/790228ffa9ae0f19


## How To Use:
refrence bottom of code file for method -- input file dir 
